---
title: Program
indexing: false
sitemap: false
---
### Program
##### Timelines are based on Korean Time (UTC+09:00).
  * 13:20 ~ 13:30: Opening
  * 13:30 ~ 14:00: Invited Talk 1, [Michiel van de Panne](./speakers.md/#michiel-van-de-panne-university-of-british-columbia-canada) 
    * "Motion Imitation as a Useful Crutch:  Natural Movement for Animation and Robotics"
        - <span style="font-size:15px">Abstract: The generation of purposeful and directable natural movement is a shared goal for animation and robotics. Computer animation methods have a long history of using motion capture data to produce high-quality kinematic motion. In contrast, for physics-based motion generation have long been split into two categories -- those that use motion capture data as a strong prior, and those that aim to generate the motion more directly from first principles. Currently, imitation of motion capture data leads to current state-of-the-art results, in motion quality, particularly when combined with reinforcement learning. However, I will argue that in the long term, imitation-based methods can be viewed as a convenient crutch while we develop better (and more directable) from-first-principles approaches. The belief here is that a combination of physics, compliant actuation, rich sensing, and highly-capable control ultimately underly natural motion.  We'll motivate these arguments with examples including brachiation, locomotion, high-jumps, getting up, and more.</span>
  * 14:00 ~ 14:30: Invited Talk 2,[Sungjoon Choi](./speakers.md/#sungjoon-choi-korea-university-korea)
    * "Robust Robot Motion Retargeting from RGB Videos"
        - <span style="font-size:15px">Abstract: Generating natural and expressive robotic motions for humanoid robots has gained considerable interest from both robotics and computer graphics communities. Recently, this phenomenon has been accelerated by the fact that more human-like robots are permeating our daily lives through applications such as interactive services or educational robots. However, in order to generate a number of natural motions for humanoid robots, a substantial amount of effort is often required to design time-stamped robotic motions by animators or artists manually. In this talk, we focus on different approaches to generating diverse and natural robotic motions from multiple sources, including motion capture data and YouTube videos, while considering pose estimation errors. </span>
        
  * 14:30 ~ 15:00: Invited Talk 3, [Eiichi Yoshida](./speakers.md/#eiichi-yoshida-tokyo-university-of-science-japan)
    * "Understanding, reproducing and synthesizing natural human motions"
        - <span style="font-size:15px">Abstract: Human-likeliness in motions is one of the critical topics in research on human-robot interaction and also biomechanics. In this presentation, I will address human motion reproduction by humanoid robots and its application to evaluation of wearable assistive device. To what extent can we make the humanoid motions as close as humans' considering intrinsic difference in structure and actuation between humans and robots? We believe posing this question is helpful for natural motion understanding. In our approach, we formulated this question as an optimization problem of motion similarity, incorporating geometric morphing and constraints of the human and humanoid. Although there are still hardware limitation, we could retarget various human motions to a humanoid motions. By extracting specific feature of hip and knee motions of a lifting task, we contributed to standardization of wearable lumbar support robots. However, reproduction of human motion is halfway to the natural motion "generation": ultimately we aim at predicting, or synthesizing natural motions for anthropomorphic systems. Having this challenge in mind, I introduce recent an ongoing project focusing on analysis and synthesis of contact motions. Some preliminary results are reported on contact estimation from  motion only by using machine learning. </span>
  * 15:00 ~ 15:30: Coffee Break
  * 15:30 ~ 16:00: Invited Talk 4, [Daniele Pucci](./speakers.md/#daniele-pucci-italiano-di-technologia-italy)
    * "Towards Human-Like Motion Generation for Humanoid Robots"
        - <span style="font-size:15px">Humanoid robots are floating base systems that resemble the human shape. So, the natural question when trying to generate trajectories for these systems is to get a degree of human likeness, although this may not be a necessary condition for the problem of trajectory generation for the system itself. This talk will present trajectory-optimisation, supervised learning, and reinforcement learning  techniques for generating motions of humanoid robots with a degree of human likeness. </span>
  * 16:00 ~ 16:30: Invited Talk 5, [Moritz Bächer](./speakers.md/#moritz-bächer-disney-research-switzerland)
    * "Retargeting Motions onto Robotic Characters"
        - <span style="font-size:15px">Abstract: Legged robots or fixed-base robotic characters are built to perform highly dynamic motions. However, it remains challenging to retarget expressive motions onto these complex systems. In this talk, I will first discuss a versatile inverse kinematics to retarget artist-specified motion onto robotic systems with kinematic loops. I will then talk about a differentiable flexible multibody dynamics that enables us to retarget motions onto soft and lightweight robotic characters while suppressing vibrations. Finally, I will discuss a technique that permits the retargeting of motion capture data onto legged systems with a novel differentiable optimal control technique, accounting for differences in proportions and mass distribution of the source of input motion and the robot.</span>
        
  * 16:30 ~ 17:00: Invited Talk 6 [Libin Liu](./speakers.md/#libin-liu-peking-university-china)
    * "Go Beyond Imitation: Generative Representation and Control of Natural Movement"
        - <span style="font-size:15px">Abstract: Generating realistic behaviors is a fundamental challenge in computer animation and a critical technique in emerging fields such as digital humans and humanoid robots. Over the past years, there has been tremendous progress in this area, driven in part by advancements in deep learning and reinforcement learning. In this talk, I will introduce our recent work on developing multi-skilled characters capable of executing natural movements. I will provide a brief overview of early efforts using state machines and control snippet scheduling but will focus more on the utilization of more powerful generative models. Particularly, I will detail how variational autoencoders and their quantized versions serve as efficient and compact representations for motion. Additionally, I will explain how we manipulate these generative models to achieve fine-grained control over the produced motion. We utilize multi-modal inputs, such as text, imagery, video, and motion demonstration, to achieve precise control of motion details. This approach not only offers an intuitive interface for users to manipulate motion but also enables seamless integration with advanced AI systems like ChatGPT.</span>
  * 17:00 ~ 17:10: Closing
